# Final Project

Hey Team,

Weâ€™ve got an exciting opportunity to showcase a **generative music tool** at an important industry conference. This is a big deal for us, and Iâ€™m counting on your creativity and skills to make it unforgettable!

Your task is to create an **amazing generative music system** that can generate music in **semi-real-time**. What does this mean? I envision a Python program that continuously generates music, sends MIDI messages to a DAW, and uses virtual instruments or samples to sonify the output. The system should run indefinitely, generating new music on the fly.

## The Challenge
Iâ€™m giving you total creative freedom to decide:

* **What to build**: The type of generative music system.
* **How to build it**: The techniques and algorithms to use.

The goal is simple: create an **awesome generative AI music experience** that produces excellent and highly creative music.

Feel also free to use pre-trained Hugging Face models if you want.


Looking for some inspiration? Here are some ideas from previous years:

* **Liquiprism**: An implementation of the [Liquiprism Paper](https://www.icad.org/websiteV2.0/Conferences/ICAD2002/proceedings/36_AlanDorin.pdf)
  * [Live DAW integration](https://www.youtube.com/shorts/J_Kz7gld5D4)
  * [Web-based demo](https://liquiprism.vercel.app/)

Feel free to explore similar approaches or develop your own unique concept!

## Deliverables
Hereâ€™s what I need by the end of the sprint:

* **1. Code Repository:**
  * A repo containing the Python 3.10 program on GitHub Classroom
  * Please write clean, professional, and well-documented code, as it will be handed over to the production team for further development if needed.

* **2. 10-minute Presentation (live):**
  * Explain the system clearly (focus on making the explanation easy to follow)
  * Show the system in action
  * I want to see how it works and get a feel for the music it generates

## Stretch Goal (Optional)

If youâ€™re feeling ambitious, consider adding user **interaction controls** to influence the generation process. For example:

* **Motion-based Interaction**: Movement influences note density.
* **Environmental Interaction**: Room temperature correlates with harmony dissonance.
* **User-interface**: A window with sliders & knobs, external MIDI controller support.

This isnâ€™t required, but experimenting with these ideas could make the system even more impressive. Remember, the skyâ€™s the limit!

## Deadlines

The final project is due by **February 16th** at midnight on GitHub Classroom
Your team will present on February 17th in class

## Final Notes
Iâ€™m confident this project will be a lot of fun and a chance to let your creativity shine. If you deliver a killer generative music system, it will grab attention at the conference and open doors for us all.

Make sure to collaborate effectivelyâ€”leverage each team memberâ€™s unique skills in music, sound design, and tech.

Good luck, and letâ€™s make something extraordinary!

Valerio, your CTO ðŸŽµðŸš€

---
